# Default experiment configuration
# This file contains all hyperparameters for a training run.
# Copy this file and modify for new experiments.

# =============================================================================
# Training Loop Settings
# =============================================================================
training:
  num_epochs: 400
  batch_size: 32
  steps_per_epoch: 512

# =============================================================================
# GRPO (Group Relative Policy Optimization) Config
# Tuned for pretrain + GRPO (see research_docs/2026-01-29_pretrain-grpo-failure-analysis.md)
# =============================================================================
grpo:
  lr: 0.000003  # 3e-6: lower LR preserves pretrained features
  num_trajectories: 16
  trajectory_depth: 16
  clip_ratio: 0.20  # wider clip ratio for pretrained init (was 0.10)
  kl_coef: 0.01  # initial value; adaptive_kl will adjust it
  entropy_coef: 0.10
  eval_every_n_epochs: 10
  ppo_steps: 1  # single step prevents policy drift (was 4)
  rollout_temperature: 1.3  # >1 increases exploration during rollouts

  # Entropy floor monitoring
  use_entropy_floor: true
  entropy_floor: 1.5
  entropy_floor_steps: 150
  entropy_floor_action: "boost"  # "warn", "stop", or "boost"
  entropy_boost_factor: 1.5

  # Adaptive KL controller
  adaptive_kl: true
  target_kl: 0.012
  kl_adapt_rate: 1.2  # slower adaptation (was 1.5)
  kl_coef_min: 0.001
  kl_coef_max: 0.1  # lower max prevents KL dominating (was 0.2)

  # Safety checks
  enable_safety_checks: true
  safety_patience_steps: 1000
  max_clip_fraction: 0.95
  min_entropy: 0.5
  max_kl_divergence: 0.08

# =============================================================================
# Transformer Model Config
# =============================================================================
transformer:
  vocab_size: 300
  embed_dim: 256
  num_layers: 4
  num_heads: 8
  action_dim: 1968

# =============================================================================
# Evaluation Config (vs Stockfish)
# =============================================================================
eval:
  games: 64
  seed: 0
  max_plies: 400
  randomize_opening: true
  opening_plies: 6

# =============================================================================
# Stockfish Config
# =============================================================================
stockfish:
  path: "/usr/games/stockfish"  # Override in colab/local as needed
  skill_level: 2
  use_elo_limit: false
  elo: 2500
  movetime_ms: 50
  threads: 1
  hash_mb: 128

# =============================================================================
# Policy Player Config (for evaluation)
# =============================================================================
policy:
  temperature: 0.8
  greedy: true
  branching_factor: 4
  search_depth: 2

# =============================================================================
# Searcher Config (optional - set to null to disable)
# =============================================================================
searcher: null
# searcher:
#   n_trajectories: 4
#   trajectory_depth: 8

# =============================================================================
# Pretraining (optional - load pretrained weights before GRPO)
# =============================================================================
pretrain:
  checkpoint_path: null  # Path to pretrained checkpoint (e.g., "checkpoints/pretrain/pretrain_final.pt")
  freeze_layers: 2       # Freeze first 2 transformer layers to preserve learned representations

# =============================================================================
# Dataset Config (Chess Start States)
# =============================================================================
dataset:
  max_steps: 512  # Should match steps_per_epoch
  phase_distribution:
    opening: 0.33
    middlegame: 0.34
    endgame: 0.33
  min_eval_cp: -200
  max_eval_cp: 200
  quality_filter: true
  stockfish_filter_depth: 4
