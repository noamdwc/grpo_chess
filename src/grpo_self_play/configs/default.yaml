# Default experiment configuration
# This file contains all hyperparameters for a training run.
# Copy this file and modify for new experiments.

# =============================================================================
# Training Loop Settings
# =============================================================================
training:
  num_epochs: 400
  batch_size: 32
  steps_per_epoch: 512
  checkpoint_every_n_epochs: 5  # Save periodic checkpoint every N epochs for crash recovery
  keep_n_checkpoints: 3         # Keep last N periodic checkpoints per run

# =============================================================================
# GRPO (Group Relative Policy Optimization) Config
# Clean run config (see research_docs/2026-02-06_loss-budget-and-monitor-analysis.md)
# =============================================================================
grpo:
  lr: 0.000001  # 1e-6: reduced because PPO signal now dominates gradient
  num_trajectories: 16
  trajectory_depth: 16
  clip_ratio: 0.20
  kl_coef: 0.001  # reduced from 0.01 (was being overridden to 0.1 by adaptive KL)
  entropy_coef: 0.0  # removed: not part of original GRPO loss, was 95% of gradient
  eval_every_n_epochs: 10
  ppo_steps: 1
  rollout_temperature: 1.3

  # Entropy floor monitoring — disabled (never triggered, see research doc)
  use_entropy_floor: false
  entropy_floor: 1.5
  entropy_floor_steps: 150
  entropy_floor_action: "boost"
  entropy_boost_factor: 1.5

  # Adaptive KL controller — disabled (saturated at max instantly, see research doc)
  adaptive_kl: false
  target_kl: 0.012
  kl_adapt_rate: 1.2
  kl_coef_min: 0.001
  kl_coef_max: 0.1

  # Safety checks
  enable_safety_checks: false
  safety_patience_steps: 1000
  max_clip_fraction: 0.95
  min_entropy: 0.5
  max_kl_divergence: 0.08

  # Teacher forcing: use Stockfish for rival moves during trajectory sampling
  teacher_forcing_prob: 0.1  # 10% of rival moves will be from Stockfish
  teacher_forcing_depth: 4

# =============================================================================
# Transformer Model Config
# =============================================================================
transformer:
  vocab_size: 300
  embed_dim: 256
  num_layers: 4
  num_heads: 8
  action_dim: 1968

# =============================================================================
# Evaluation Config (vs Stockfish)
# =============================================================================
eval:
  games: 64
  seed: 0
  max_plies: 400
  randomize_opening: true
  opening_plies: 6

# =============================================================================
# Stockfish Config
# =============================================================================
stockfish:
  path: "/usr/games/stockfish"  # Override in colab/local as needed
  skill_level: 2
  use_elo_limit: false
  elo: 2500
  movetime_ms: 50
  threads: 1
  hash_mb: 128

# =============================================================================
# Policy Player Config (for evaluation)
# =============================================================================
policy:
  temperature: 0.8
  greedy: true
  branching_factor: 4
  search_depth: 2

# =============================================================================
# Searcher Config (optional - set to null to disable)
# =============================================================================
searcher: null
# searcher:
#   n_trajectories: 4
#   trajectory_depth: 8

# =============================================================================
# Pretraining (optional - load pretrained weights before GRPO)
# =============================================================================
pretrain:
  checkpoint_path: null  # Path to pretrained checkpoint (e.g., "checkpoints/pretrain/pretrain_final.pt")
  freeze_layers: 2       # Freeze first 2 transformer layers to preserve learned representations

# =============================================================================
# Dataset Config (Chess Start States)
# =============================================================================
dataset:
  max_steps: 512  # Should match steps_per_epoch
  phase_distribution:
    opening: 0.33
    middlegame: 0.34
    endgame: 0.33
  min_eval_cp: -200
  max_eval_cp: 200
  quality_filter: true
  stockfish_filter_depth: 4
