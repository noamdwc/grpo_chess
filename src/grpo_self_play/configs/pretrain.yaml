# Pretraining configuration for chess model
# This file contains hyperparameters for supervised pretraining on Lichess games.
#
# Usage:
#   python -m src.grpo_self_play.pretrain.pretrain --config pretrain.yaml

# =============================================================================
# Pretraining Settings
# =============================================================================
pretrain:
  lr: 0.0001                    # Learning rate (higher than GRPO fine-tuning)
  batch_size: 4096              # Batch size for pretraining
  num_epochs: 180                # Number of passes through the dataset
  warmup_steps: 1000            # Linear warmup steps
  weight_decay: 0.01            # AdamW weight decay
  max_grad_norm: 1.0            # Gradient clipping
  checkpoint_dir: "checkpoints/pretrain"
  resume_from: null             # Path to resume from (optional)
  use_wandb: true
  wandb_project: "chess-grpo-pretrain"
  label_smoothing: 0.1          # Prevents overconfidence
  num_workers: 4                # DataLoader workers
  val_check_interval: 0.1       # Validate every 10% of epoch

# =============================================================================
# Dataset Settings (Lichess games from HuggingFace)
# =============================================================================
dataset:
  min_elo: 1800                 # Minimum player rating to include
  max_samples: 5000000          # Max samples per epoch (null = unlimited)
  skip_first_n_moves: 5         # Skip opening moves (book territory)
  skip_last_n_moves: 5          # Skip endgame/resignation moves
  sample_positions_per_game: 3  # Positions to sample from each game
  buffer_size: 10000            # Shuffle buffer size for streaming
  filter_abandoned: true        # Skip abandoned games
  dataset_name: "Lichess/standard-chess-games"
  split: "train"                # Dataset split to use
  is_eval: false                # False for training, True for evaluation
  eval_fraction: 0.05           # 5% of games held out for evaluation

# =============================================================================
# Transformer Model Config (should match GRPO training)
# =============================================================================
transformer:
  vocab_size: 300
  embed_dim: 256
  num_layers: 4
  num_heads: 8
  action_dim: 1968
